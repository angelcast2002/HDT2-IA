{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Preguntas Teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "\n",
    "El proceso de decisión de markov (MDP) es un modelo matemático usado para tomar desiciones en situaciones en donde los resultados son parcialmente aleatorios y en el que hay un bajo control del tomador de desiciones. Suele ser usado en teoría de control, investigación de operaciones, economía y en general muchos campos en donde el aprendizaje automático y por refuerzo entra en juego. \n",
    "\n",
    "Proporciona un marco formal para modelar la toma de decisiones en situaciones en donde los resultados dependen tanto de las acciones de un agente como de elementos aleatorios. El objetivo del modelo es encontrar una política, que suele ser definida como *(π)* que es una función que asigna a cada estado la acción a tomar, de manera que se maximice alguna medida de ganancia a largo plazo. El \"endgame\" es encontrar una política adecuada que garantice la recompensa total esperada. \n",
    "\n",
    "Los componentes son:\n",
    "- Conjunto de estados (S): es el conjunto de todos los estados posibles en los que se pueda encontrar el sistema. Un estado representa una situación específica del mundo que el agente está tratando de modelar. \n",
    "- Conjunto de acciones (A): es el conjunto de las acciones que el agente puede elegir. Las acciones son las decisiones o movimientos que el agente puede realizar.\n",
    "- Función de transición (P): es una función de probabilidad, dado un estado actual y una acción tomada por el agente, la función de transición devuelve la probabilidad de transitar a cualquier otro estado. Se suele definir como *P(s'|s,a)*, en otras palabras, la probabilidad de llegar al estado s' desde el estado s tomando en cuenta la acción a. \n",
    "- Función de recompensa (R): es un componente fundamental del modelo, asigna una recompensa o costo a cada transición entre estados o a la llegada de ciertos estados. Se denota como *R(s, a, s')* o en otras palabras la recompensa por transitar del estado s al estado s' mediante la acción a. \n",
    "- Factor de descuento (γ): es un parámetro que determina la importancia de las recompensas futuras. Un factor de descuento cercano a 0 hace que el agente sea miópico, es decir que considera únicamente el factor de recompensa, mientras que un factor cercano a 1 hace que se valoren más las recompensas futuras. Esto ayuda a garantizar que la suma de las recompensas no sea inficita y a manejar la incertidumbre de recompensas futuras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.\n",
    "\n",
    "La política define la estrategia que un agente seguirá para seleccionar las acciones en diferentes estados, buscando maximizar la recompensa total esperada. \n",
    "\n",
    "La evaluación de políticas hace referencia al cálculo de la efectuvidad de una política específica, determinando la recompensa esperada de seguirla en cada estado. \n",
    "\n",
    "En base a los resultados obtenidos por la evaluación de políticas, la mejora de políticas es el proceso de ajustar la política para aumentar las recompensas futuras esperadas, seleccionando acciones que maximicen el valor esperado de acuerdo a la evaluación. \n",
    "\n",
    "La iteración política combina los resultados en evaluación y mejora de forma iterativa, es decir, que se repiten hasta un valor dado. El enfoque iterativo asegura una mejora continua. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "\n",
    "El factor de descuento es un parámetro que determina la importancia de las recompensas futuras en comparación con las inmediatas, variando entre 0 y 1. Un valor cercano a 0 hace que el agente valore mas las recompensas inmediatas, mientras que un valor cercano a 1 hace que el agente valore más las recompensas futuras. El parámetro es crucial para equilibrar el modelo y formar la estrategia de toma de desiciones del agente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP\n",
    "\n",
    "La iteración de valores busca directamente la función de valor óptima actualizando los valores de todos los estados hasta que se llegue a la convergencia para encontrar la política óptima. La iteración de políticas se enfoca en modificar y evaluar la política original, ajustándola hasta encontrar la política óptima. En mi opinión, el método de iteración de políticas suele tener mejores resultados en problemas mas grandes o complejos, debido a que requiere menos iteraciones porque se modifica desde un enfoque mas amplio.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.\n",
    "\n",
    "Resolver problemas complejos donde se deben tomar decisiones secuenciales, como en los juegos o en la planificación de rutas puede ser desafiante cuando las desiciones suelen ser muy numerosas. Esto se debe principalmente a dos obstáculos. \n",
    "\n",
    "Primero, la dificultad aumenta exponencialmente a medida que se agregan mas opcinoes y resultados. Segundo, calcular la mejor decisión en cada paso puede requerrir una cantidad enorme de tiempo y de recursos computacionales. \n",
    "\n",
    "Se han desarrollado distintas estrategias para afrontar estos retos. Una de las estrategias es simplificar los problemas, agrupando situaciones slimilares o dividiendo el problema en sub problemas mas pequeños. Otra de las estrategias es delimitar las situaciones para encontrar soluciones buenas sin necesitar revisar cada opción existente.\n",
    "\n",
    "Es importante decir que estas soluciones no buscan la perfección de los resultados, sino las maneras mas prácticas y eficientes de brindar buenos resultados. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Preguntas Analíticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones para la toma de decisiones\n",
    "\n",
    "La propiedad de Markov asume que el futuro es independiente del pasado, dado el presente. Es decir, el estado actual captura toda la información relevante para la futura toma de desiciones. Sin embargo, en muchos escenarios reales esta suposición es demasiado restrictiva. \n",
    "\n",
    "Por ejemplo, si el historial de acciones afecta situaciones con información oculta que no se refleja en el estado actual, la propiedad de Markov no puede sostenerse correctamente. \n",
    "\n",
    "Esta es una de las limitaciones mas grandes del modelo en pro blemas complejos. En estos casos, las decisiones basadas en MDP podrían ser inadecuadas, lo que sugiere la necesidad de modelos más sofisticados o que al menos tomen en cuenta más situaciones para manejar la información de manera mas efectiva, reduciendo la incertidumbre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice estrategias para una toma de decisiones sólida en entornos inciertos.\n",
    "\n",
    "Tomar en cuenta la incertidumbre al implementar el modelo de toma de decisiones basado en Markov presenta una serie de desafios importantes, sobre todo en entornos dinámicos en donde las transiciones y recompensas no se pueden predecir con exactitud. Esta incertidumbre puede llevar a subestimar riesgos o sobrevalorar beneficios, afectando la eficacia de las decisiones. \n",
    "\n",
    "Para enfrentar esto, se pueden emplear estrategias como la \"optimización robusta\", que busca soluciones eficaces bajo muchas condiciones. Otra de las soluciones es utilizar la estrategia del aprendizaje por refuerzo, que permite adaptarse y mejorar continuamente sus decisiones a través de las experiencias anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Preguntas Prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrolle un agente básico capaz de resolver un problema simplificado del Proceso de Decisión de Markov (MDP). Considere utilizar un ejemplo bien conocido como el entorno 'Frozen Lake'. Proporcione el código Python para el proceso de toma de decisiones del agente basándose únicamente en los principios de los procesos de decisión de Markov. Recuerde que para este tipo de problema, el ambiente es una matriz de 4x4, y las acciones, pueden ser moverse hacia arriba, abajo, derecha, izquierda.\n",
    "\n",
    "\n",
    "Considere que el punto inicial siempre estará en la esquina opuesta del punto de meta. Es decir, puede tener hasta 4 configuraciones diferentes. Por ejemplo, el punto inicial puede estar en la coordenada (0, 0) y el punto de meta en la coordenada en la coordenada (3, 3). Además, la posición de los hoyos debe ser determinada aleatoriamente y no debe superar el ser más de 3. Es decir, si aleatoriamente se decide que sean 2 posiciones de hoyo, las coordenadas de estas deben ser determinadas de forma aleatoria.\n",
    "\n",
    "\n",
    "Asegúrese de usar **“seed”** para que sus resultados sean consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox.example\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 4\n",
    "P = np.array([\n",
    "    [[0, 1, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n",
    "    [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]],\n",
    "    [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 0, 1]],\n",
    "    [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\n",
    "])\n",
    "\n",
    "R = np.array([\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]] # Se coloca un premio en la esquina inferior derecha ya que es el estado final\n",
    "])\n",
    "\n",
    "np.random.seed(4)\n",
    "hoyos = []\n",
    "\n",
    "cantidadHoyos = 3\n",
    "for i in range(cantidadHoyos):\n",
    "    hoyos.append(np.random.randint(0, n - 1, 2)) # Se coloca un hoyo en una posicion aleatoria\n",
    "\n",
    "for hoyo in hoyos:\n",
    "    x, y = hoyo # Se coloca un castigo (recompensa negativa) en la posicion del hoyo\n",
    "    R[x, y, :] = -100\n",
    "\n",
    "mdp = mdptoolbox.mdp.ValueIteration(P, R, 0.8, initial_value=0) # Se crea el objeto mdp con los parametros necesarios\n",
    "\n",
    "mdp.run()   # Se ejecuta el algoritmo de value iteration\n",
    "\n",
    "print(mdp.policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política óptima (2, 2, 3, 3) indica lo siguiente:\n",
    "\n",
    "1. En el estado 2 (estado inicial), la acción óptima es tomar la acción 2.\n",
    "2. En el estado 1, la acción óptima es tomar la acción 2.\n",
    "3. En el estado 3, la acción óptima es tomar la acción 3.\n",
    "4. En el estado 3, la acción óptima es tomar la acción 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La biblioteca `mdptoolbox` utiliza el algoritmo de iteración de valor para resolver problemas de procesos de decisión de Markov (MDP). Este algoritmo funciona de la siguiente manera:\n",
    "\n",
    "**Inicialización:**\n",
    "- Se asigna un valor inicial a cada estado.\n",
    "\n",
    "**Iteración:**\n",
    "- Para cada estado, se calcula su nuevo valor basado en las recompensas inmediatas y los valores de los estados alcanzables desde ese estado.\n",
    "- Este proceso se repite hasta que los valores de los estados convergen, es decir, hasta que dejen de cambiar significativamente en cada iteración.\n",
    "\n",
    "**Selección de acciones óptimas:**\n",
    "- Se elige la acción óptima en cada estado según los nuevos valores calculados.\n",
    "\n",
    "**Convergencia:**\n",
    "- El algoritmo converge cuando los valores de los estados convergen y la política óptima se estabiliza, ya no posee cambios.\n",
    "\n",
    "**Salida:**\n",
    "- Se devuelve la política óptima, que indica la mejor acción a tomar en cada estado para maximizar la recompensa total esperada a largo plazo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
