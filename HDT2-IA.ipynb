{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Preguntas Teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala? Discuta los enfoques potenciales para abordar estos desafíos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Preguntas Analíticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones para la toma de decisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice estrategias para una toma de decisiones sólida en entornos inciertos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Preguntas Prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrolle un agente básico capaz de resolver un problema simplificado del Proceso de Decisión de Markov (MDP). Considere utilizar un ejemplo bien conocido como el entorno 'Frozen Lake'. Proporcione el código Python para el proceso de toma de decisiones del agente basándose únicamente en los principios de los procesos de decisión de Markov. Recuerde que para este tipo de problema, el ambiente es una matriz de 4x4, y las acciones, pueden ser moverse hacia arriba, abajo, derecha, izquierda.\n",
    "\n",
    "\n",
    "Considere que el punto inicial siempre estará en la esquina opuesta del punto de meta. Es decir, puede tener hasta 4 configuraciones diferentes. Por ejemplo, el punto inicial puede estar en la coordenada (0, 0) y el punto de meta en la coordenada en la coordenada (3, 3). Además, la posición de los hoyos debe ser determinada aleatoriamente y no debe superar el ser más de 3. Es decir, si aleatoriamente se decide que sean 2 posiciones de hoyo, las coordenadas de estas deben ser determinadas de forma aleatoria.\n",
    "\n",
    "\n",
    "Asegúrese de usar **“seed”** para que sus resultados sean consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mdptoolbox.example\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 4\n",
    "P = np.array([\n",
    "    [[0, 1, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n",
    "    [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0]],\n",
    "    [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 0, 1]],\n",
    "    [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\n",
    "])\n",
    "\n",
    "R = np.array([\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],\n",
    "    [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]]\n",
    "])\n",
    "\n",
    "np.random.seed(4)\n",
    "hoyos = []\n",
    "\n",
    "cantidadHoyos = 3\n",
    "for i in range(cantidadHoyos):\n",
    "    hoyos.append(np.random.randint(0, n - 1, 2))\n",
    "\n",
    "for hoyo in hoyos:\n",
    "    x, y = hoyo\n",
    "    R[x, y, :] = -100\n",
    "\n",
    "mdp = mdptoolbox.mdp.ValueIteration(P, R, 0.8, initial_value=0)\n",
    "\n",
    "mdp.run()\n",
    "\n",
    "print(mdp.policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política óptima (2, 2, 3, 2) indica lo siguiente:\n",
    "\n",
    "1. En el estado 2 (estado inicial), la acción óptima es tomar la acción 2.\n",
    "2. En el estado 1, la acción óptima es tomar la acción 2.\n",
    "3. En el estado 2, la acción óptima es tomar la acción 3.\n",
    "4. En el estado 3, la acción óptima es tomar la acción 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
